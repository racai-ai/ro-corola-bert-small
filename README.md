# A small Romanian BERT model
This is the repository with which to build a small Romanian BERT model based on the [CoRoLa corpus](https://corola.racai.ro/).  
It needs the Romanian aware WordPiece tokenizer that is avaiable [here](https://github.com/racai-ai/ro-wordpiece-tokenizer).

BERT parameters are:
* maximum sequence length `256`
* `L = 4` (4 stacked layers)
* `Î— = 256` (size of the hidden layer)
* number of attention heads `8`

The model is pretrained with the MLM objective, `mlm_probability=0.15`.  
It takes approximately 30 days to train on an NVIDIA Quadro RTX 8000 with 48GB of RAM.  
